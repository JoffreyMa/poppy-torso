{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Poppy Mimic Movements with SAC \n",
    "\n",
    "Let's start to work on our poppy robot project!\\\n",
    "The goal is to mimic the movement of a person with a robot.\\\n",
    "More specifically we have a video of movements of the upper body of a person and a simulation of a robot torso.\n",
    "\n",
    "Instead of using inverse kinematics, we are going to use reinforcement learning, more specifically using SAC for Soft Actor Critic:\n",
    "\n",
    "https://stable-baselines3.readthedocs.io/en/master/modules/sac.html\n",
    "\n",
    "From the introductory notebook of this project, we have : \n",
    "* The action is the joint positions given to each of the motors.\n",
    "* The observation are the cartesian positions that can be accessed by commands like poppy.l_arm_chain.position.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from pypot.creatures import PoppyTorso\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "from pypot.creatures.ik import IKChain\n",
    "from pypot.primitive.move import Move\n",
    "from pypot.primitive.move import MovePlayer\n",
    "# To install Stable baselines I needed to downgrade wheel to 0.38.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"./gym-examples/\") # to be ale to import from the gym_examples folder, which I simply put inside this project\n",
    "import gym_examples\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import SAC"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instanciate the robot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypot import vrep\n",
    "vrep.close_all_connections()\n",
    "poppy = PoppyTorso(simulator='vrep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = None\n",
    "targets = None\n",
    "smoothed_targets = None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of pursuing with the robot, we build the environment necessary to implement RL with the CoppeliamSim simulation.\\\n",
    "I found this repository to draw inspiration from : https://github.com/chauby/CoppeliaSimRL \\\n",
    "Unfortunately it works with Gym instead of Gymnasium the newer version. \n",
    "\n",
    "I cloned the gym-example in another folder, one folder up compare to the poppy-torso we are currently in : https://gymnasium.farama.org/tutorials/gymnasium_basics/environment_creation/#subclassing-gymnasium-env\n",
    "\n",
    "There are kind of two parts in our environment, one for the agent and one for the target, which is moving too.\\\n",
    "Once we are done with setting the agent, let's try and see how it is going."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.1.0 (SDL 2.0.16, Python 3.9.5)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "Initialiazing PoppyTorsoEnv\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('gym_examples/PoppyTorso-v0')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, now let's try to take a step in the environment. Hopefully our robot will move."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'agent': array([[ 0.11508345, -0.16457885,  0.05731346],\n",
       "        [-0.10222124, -0.17925963,  0.07029861]]),\n",
       " 'target': tensor([[ 0.1619, -0.1774,  0.0378],\n",
       "         [-0.1155, -0.2014,  0.0246]])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First a reset is needed !\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfps = 10\\nmove_motors = [m.name for m in poppy.motors]\\nfor t in np.linspace(0.02,3,int(3*fps)):\\n    new_positions = {}\\n    for motor in move_motors:\\n        # decide for each timestep and each motor a joint angle and a velocity\\n        new_positions[motor] = [20*np.sin(t), 0.0]\\naction = new_positions\\nenv.step(action)\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "fps = 10\n",
    "move_motors = [m.name for m in poppy.motors]\n",
    "for t in np.linspace(0.02,3,int(3*fps)):\n",
    "    new_positions = {}\n",
    "    for motor in move_motors:\n",
    "        # decide for each timestep and each motor a joint angle and a velocity\n",
    "        new_positions[motor] = [20*np.sin(t), 0.0]\n",
    "action = new_positions\n",
    "env.step(action)\n",
    "'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our robot moves but it does randomly which is normal for now. We want to know the targets that our robot has to follow. Let's focus  on some end effectors like the hands, and set those as target, we won't pay attention to the rest for now. \n",
    "\n",
    "Hence we need to implement the necessary code in the environment to get the targets.\n",
    "\n",
    "**time passing**\n",
    "\n",
    "It is done. I'd like to know at each step the observations. Also I'd like to know it for the initial step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor _ in range(5):\\n    env.reset()\\n    done = False\\n    while not done:\\n        action = env.action_space.sample()\\n        obs, reward, done, info = env.step(action)\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "for _ in range(5):\n",
    "    env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        obs, reward, done, info = env.step(action)\n",
    "'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also set up a reward related to the inverse distance till the target.\n",
    "Now I still don't know if what I'm doing is more or less correct, but I'll try to apply a SAC algorithm nonetheless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m env\u001b[39m.\u001b[39mreset()\n\u001b[1;32m      2\u001b[0m model \u001b[39m=\u001b[39m SAC(\u001b[39m'\u001b[39m\u001b[39mMultiInputPolicy\u001b[39m\u001b[39m'\u001b[39m, env, verbose\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m model\u001b[39m.\u001b[39;49mlearn(total_timesteps\u001b[39m=\u001b[39;49m\u001b[39m10000\u001b[39;49m)\n\u001b[1;32m      4\u001b[0m obs \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mreset()\n\u001b[1;32m      5\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1000\u001b[39m):\n",
      "File \u001b[0;32m~/Projets/mastere_ia/cours/IA705 - Apprentissage pour la robotique/poppy-torso/venv/lib/python3.9/site-packages/stable_baselines3/sac/sac.py:302\u001b[0m, in \u001b[0;36mSAC.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlearn\u001b[39m(\n\u001b[1;32m    294\u001b[0m     \u001b[39mself\u001b[39m: SelfSAC,\n\u001b[1;32m    295\u001b[0m     total_timesteps: \u001b[39mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    300\u001b[0m     progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    301\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m SelfSAC:\n\u001b[0;32m--> 302\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mlearn(\n\u001b[1;32m    303\u001b[0m         total_timesteps\u001b[39m=\u001b[39;49mtotal_timesteps,\n\u001b[1;32m    304\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[1;32m    305\u001b[0m         log_interval\u001b[39m=\u001b[39;49mlog_interval,\n\u001b[1;32m    306\u001b[0m         tb_log_name\u001b[39m=\u001b[39;49mtb_log_name,\n\u001b[1;32m    307\u001b[0m         reset_num_timesteps\u001b[39m=\u001b[39;49mreset_num_timesteps,\n\u001b[1;32m    308\u001b[0m         progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[1;32m    309\u001b[0m     )\n",
      "File \u001b[0;32m~/Projets/mastere_ia/cours/IA705 - Apprentissage pour la robotique/poppy-torso/venv/lib/python3.9/site-packages/stable_baselines3/common/off_policy_algorithm.py:311\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    308\u001b[0m callback\u001b[39m.\u001b[39mon_training_start(\u001b[39mlocals\u001b[39m(), \u001b[39mglobals\u001b[39m())\n\u001b[1;32m    310\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 311\u001b[0m     rollout \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollect_rollouts(\n\u001b[1;32m    312\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv,\n\u001b[1;32m    313\u001b[0m         train_freq\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_freq,\n\u001b[1;32m    314\u001b[0m         action_noise\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maction_noise,\n\u001b[1;32m    315\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[1;32m    316\u001b[0m         learning_starts\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlearning_starts,\n\u001b[1;32m    317\u001b[0m         replay_buffer\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreplay_buffer,\n\u001b[1;32m    318\u001b[0m         log_interval\u001b[39m=\u001b[39;49mlog_interval,\n\u001b[1;32m    319\u001b[0m     )\n\u001b[1;32m    321\u001b[0m     \u001b[39mif\u001b[39;00m rollout\u001b[39m.\u001b[39mcontinue_training \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m    322\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/Projets/mastere_ia/cours/IA705 - Apprentissage pour la robotique/poppy-torso/venv/lib/python3.9/site-packages/stable_baselines3/common/off_policy_algorithm.py:543\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.collect_rollouts\u001b[0;34m(self, env, callback, train_freq, replay_buffer, action_noise, learning_starts, log_interval)\u001b[0m\n\u001b[1;32m    540\u001b[0m actions, buffer_actions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sample_action(learning_starts, action_noise, env\u001b[39m.\u001b[39mnum_envs)\n\u001b[1;32m    542\u001b[0m \u001b[39m# Rescale and perform action\u001b[39;00m\n\u001b[0;32m--> 543\u001b[0m new_obs, rewards, dones, infos \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(actions)\n\u001b[1;32m    545\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mnum_envs\n\u001b[1;32m    546\u001b[0m num_collected_steps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/Projets/mastere_ia/cours/IA705 - Apprentissage pour la robotique/poppy-torso/venv/lib/python3.9/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:163\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[39mStep the environments with the given action\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \n\u001b[1;32m    159\u001b[0m \u001b[39m:param actions: the action\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[39m:return: observation, reward, done, information\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep_async(actions)\n\u001b[0;32m--> 163\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep_wait()\n",
      "File \u001b[0;32m~/Projets/mastere_ia/cours/IA705 - Apprentissage pour la robotique/poppy-torso/venv/lib/python3.9/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:54\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep_wait\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m VecEnvStepReturn:\n\u001b[1;32m     53\u001b[0m     \u001b[39mfor\u001b[39;00m env_idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_envs):\n\u001b[0;32m---> 54\u001b[0m         obs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_rews[env_idx], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_dones[env_idx], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_infos[env_idx] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menvs[env_idx]\u001b[39m.\u001b[39;49mstep(\n\u001b[1;32m     55\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mactions[env_idx]\n\u001b[1;32m     56\u001b[0m         )\n\u001b[1;32m     57\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_dones[env_idx]:\n\u001b[1;32m     58\u001b[0m             \u001b[39m# save final observation where user can get it, then reset\u001b[39;00m\n\u001b[1;32m     59\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_infos[env_idx][\u001b[39m\"\u001b[39m\u001b[39mterminal_observation\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m obs\n",
      "File \u001b[0;32m~/Projets/mastere_ia/cours/IA705 - Apprentissage pour la robotique/poppy-torso/venv/lib/python3.9/site-packages/stable_baselines3/common/monitor.py:95\u001b[0m, in \u001b[0;36mMonitor.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mneeds_reset:\n\u001b[1;32m     94\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTried to step environment that needs reset\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 95\u001b[0m observation, reward, done, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m     96\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrewards\u001b[39m.\u001b[39mappend(reward)\n\u001b[1;32m     97\u001b[0m \u001b[39mif\u001b[39;00m done:\n",
      "File \u001b[0;32m~/Projets/mastere_ia/cours/IA705 - Apprentissage pour la robotique/poppy-torso/venv/lib/python3.9/site-packages/gym/wrappers/time_limit.py:18\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[1;32m     15\u001b[0m     \u001b[39massert\u001b[39;00m (\n\u001b[1;32m     16\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     ), \u001b[39m\"\u001b[39m\u001b[39mCannot call env.step() before calling reset()\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> 18\u001b[0m     observation, reward, done, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m     19\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     20\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m~/Projets/mastere_ia/cours/IA705 - Apprentissage pour la robotique/poppy-torso/./gym-examples/gym_examples/envs/poppy_torso_env.py:86\u001b[0m, in \u001b[0;36mPoppyTorsoEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[1;32m     81\u001b[0m     \u001b[39m#print(\"Taking a step in the PoppyTorsoEnv\")\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \n\u001b[1;32m     83\u001b[0m     \u001b[39m# State the movement to do based on action\u001b[39;00m\n\u001b[1;32m     84\u001b[0m     \u001b[39mfor\u001b[39;00m m,a \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpoppy\u001b[39m.\u001b[39mmotors, action):\n\u001b[1;32m     85\u001b[0m         \u001b[39m# Send the motor commands to the robot\u001b[39;00m\n\u001b[0;32m---> 86\u001b[0m         m\u001b[39m.\u001b[39;49mgoto_position(a, \u001b[39m1\u001b[39;49m, wait\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     88\u001b[0m \u001b[39m    \u001b[39m\u001b[39m'''\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[39m    # State the movement to do based on action\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[39m    move = Move(freq=self.fps) # could be great if possible to collect the moves later on\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[39m    print(\"joint positions of the move \",(move._timed_positions))\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[39m    '''\u001b[39;00m\n\u001b[1;32m    101\u001b[0m     \u001b[39m# Get observations, that is to say the cartesian position of the joints\u001b[39;00m\n",
      "File \u001b[0;32m~/Projets/mastere_ia/cours/IA705 - Apprentissage pour la robotique/poppy-torso/venv/lib/python3.9/site-packages/pypot/dynamixel/motor.py:263\u001b[0m, in \u001b[0;36mDxlMotor.goto_position\u001b[0;34m(self, position, duration, control, wait)\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgoal_position \u001b[39m=\u001b[39m position\n\u001b[1;32m    262\u001b[0m     \u001b[39mif\u001b[39;00m wait:\n\u001b[0;32m--> 263\u001b[0m         time\u001b[39m.\u001b[39;49msleep(duration)\n\u001b[1;32m    265\u001b[0m \u001b[39melif\u001b[39;00m control \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mlinear\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    266\u001b[0m     goto_linear \u001b[39m=\u001b[39m GotoLinear(\u001b[39mself\u001b[39m, position, duration)\n",
      "File \u001b[0;32m~/Projets/mastere_ia/cours/IA705 - Apprentissage pour la robotique/poppy-torso/venv/lib/python3.9/site-packages/pypot/vrep/__init__.py:49\u001b[0m, in \u001b[0;36mvrep_time.sleep\u001b[0;34m(self, t)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_time() \u001b[39m<\u001b[39m t0:\n\u001b[1;32m     48\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m sys_time\u001b[39m.\u001b[39;49msleep(\u001b[39m0.01\u001b[39;49m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "model = SAC('MultiInputPolicy', env, verbose=1)\n",
    "model.learn(total_timesteps=10000)\n",
    "obs = env.reset()\n",
    "for i in range(1000):\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    if done:\n",
    "      obs = env.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the agent\n",
    "model.save(\"sac_arm\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
